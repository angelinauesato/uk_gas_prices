# ğŸš€ UK Fuel Price Archiver
This project implements a robust, end-to-end data pipeline to collect, process, transform, and serve historical and current fuel price data for various retailers across the UK. Leveraging a modern data stack (Airflow, Spark, dbt, MinIO, PostgreSQL), the pipeline ensures data quality, efficiency, and provides a reliable foundation for analytics and reporting.
![data_pipeline_uk_fuel](https://github.com/user-attachments/assets/6a7e0888-3183-40d3-97b7-afcbf451de5b)

## Architecture
The pipeline follows a Medallion Architecture pattern (Bronze, Silver, Gold layers) and uses a Star Schema for its final data model.
# ğŸ›  Tech Stack
* **Airflow:** Orchestrates the entire pipeline, managing dependencies, scheduling, and error handling.
* **Spark:** Processes raw data, reads from S3 (MinIO), and writes to PostgreSQL.
* **dbt (Data Build Tool):** Transforms, tests, and documents data within PostgreSQL, creating a clean Star Schema.
* **MinIO (S3-compatible storage):** Stores raw JSON data collected from external sources.
* **PostgreSQL:** Serves as the data warehouse for staged, dimensional, and factual data.
* **Docker Compose:** Manages the local development environment for all services.

# ğŸ“‚ Project Structure
```
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ fuel_fetch_data/
â”‚       â””â”€â”€ api_request.py    # Main DAG file
|â”€â”€ dbt/
|     â””â”€â”€fuel_project/
|        â””â”€â”€ analyses/
|        â””â”€â”€ macros/
|        â””â”€â”€ models/
|        â””â”€â”€ seeds/
|        â””â”€â”€ snapshots/
|        â””â”€â”€ tests/
|        â”€â”€ dbt_project.yml
|        â”€â”€ profiles.yml # ignored and not pushed to the repo
â”œâ”€â”€ docker-compose.yaml       # Airflow & MinIO services
|â”€â”€ .env                      # ignored and not pushed to the repo
â””â”€â”€ README.md
```
# ğŸš€ Getting Started
  1. Prerequisites:<br />
     - Docker and Docker Compose installed.
     - Port 8080 (Airflow), 8081( dbt lineage) and 9001 (MinIO Console) are available.
     - Python 3.8+
  2. Configuration Parameters: <br />
     Ensure your .env file contains the following variables: <br />
      ```
        AIRFLOW_UID=50000
        AIRFLOW_USER=airflow
        AIRFLOW_PASSWORD=airflow
        AIRFLOW_GID=0
        POSTGRES_USER={USER}
        POSTGRES_PASSWORD={PASSWORD}
        POSTGRES_DB=airflow
        # Airflow UI Credentials
        AIRFLOW_WWW_USER_USERNAME={AIRFLOW_USER}
        AIRFLOW_WWW_USER_PASSWORD={AIRFLOW_PASSWORD}
        MINIO_ROOT_USER={MINIO_ROOT_USER}
        MINIO_ROOT_PASSWORD={MINIO_ROOT_PASSWORD}
        # python3 -c "import base64, os; print(base64.urlsafe_b64encode(os.urandom(32)).decode())"
        FERNET_KEY={RUN_PYTHON_COMMENTED_CODE_ABOVE}
    
        ```
  3. Launch the Environment
     ```
     docker compose -p uk_gas_prices up -d
     ```
  4. Setup Airflow Connections <br/>
     - To allow Airflow to talk to MinIO, create a connection in the Airflow UI (Admin -> Connections):
       - Conn Id: aws_s3_gas_prices
       - Conn Type: Amazon S3
       - Extra:
         ```
         {"endpoint_url": "http://minio:9000", "aws_access_key_id": "{MINIO_ROOT_USER}", "aws_secret_access_key": "{MINIO_ROOT_PASSWORD}"<br />
         ```
       **Note:** You will need to replace: {MINIO_ROOT_USER} and {MINIO_ROOT_PASSWORD} with your information, from **.env** file.
# ğŸ•’ Scheduling<br />
The DAG is configured to run daily at **9:00 AM UTC** using the cron expression 0 9 * * *.
## Running the Pipeline

1.  **Unpause the `fuel_prices_unified_pipeline` DAG** in the Airflow UI.
2.  **Trigger the DAG manually** from the Airflow UI.

The DAG will:
* Scrape fuel price data for multiple UK retailers in parallel using Spark.
* Load raw data into MinIO (S3).
* Transform and model the data using dbt, creating dimensional and factual tables in PostgreSQL.
* Run data quality tests using dbt.
* Update dbt documentation.
* Send an email notification upon successful completion or failure.
## ğŸ“Š Data Coverage
|  Retailer     |   Status  |      Note                             |
|---------------|-----------|---------------------------------------|
| Applegreen    | âœ… Active | Reliable JSON endpoint.               |
| Asda          | âœ… Active | Direct government-mandated feed.      |
| BP            | âœ… Active | Requires custom User-Agent headers.   |
| Esso          | âœ… Active | Hosted on specialized data subdomain. |
| Shell         | âŒ Blocked| Protected by Akamai Bot Manager (403).|
# ğŸ§ª Testing the Pipeline <br />
You can manually trigger a test run inside the scheduler container:
```
docker exec -it <container_id> airflow dags test uk_fuel_price_archive_v1 2026-01-24
```

# ğŸ’¡ dbt Documentation
##  Lineage: <br />
To view the data lineage and documentation generated by dbt:

1.  Ensure the `airflow-webserver` port `8081` is mapped in `docker-compose.yaml`.
2.  Run the dbt docs server:
    ```bash
    docker compose exec airflow-webserver dbt docs serve --project-dir /opt/airflow/dbt/fuel_project --profiles-dir /opt/airflow/dbt/fuel_project --port 8081
    ```
3.  Open your browser to `http://localhost:8081`.

<img width="1783" height="690" alt="Screenshot 2026-01-31 at 6 43 37â€¯PM" src="https://github.com/user-attachments/assets/da41b230-744b-4e59-9bfe-d20264f6bf1b" />


# ğŸ“œ Roadmap:<br />
- [ x ] Ingest data from retailers and save it inside of MinIO bucket.
- [ x ] Add a transformation layer to convert JSON raw layer (database in Postgres).
- [ x ] Update pipeline to trigger the transformation right after the ingestion of the JSON files.
- [ x ] Save data. Postgres. We have **raw** database: uk_gas_prices_raw and **prod** database: fuel_prices_prod
- [ x ] Update the pipeline to run dbt model to update the data.
- [ x ] Build a Slack or e-mail notification task for failure alerts.
- [ ] Expand data sources to include more retailers or real-time APIs.
- [ ] Data Archiving - moves old records (older than 30 days) into a long-term storage table
- [ ] Integration with a dedicated BI tool (e.g., Superset, Metabase).

# ğŸ” Troubleshooting & Known Issues
1. Shell 403 Forbidden Error <br />
**Issue:** The Shell API returns a 403 Forbidden error despite correct headers and session handling. <br />
**Cause:** Shell utilizes Akamai Bot Manager which performs TLS Fingerprinting. It identifies the Python requests library at the network handshake level as a non-browser entity.<br />
**Workaround:** Currently, Shell data is skipped to maintain pipeline health. Future versions may implement another solution.<br />
2. MinIO Connectivity<br />
**Issue:** botocore.exceptions.EndpointConnectionError <br />
**Solution:** Ensure  to add the connection in Airflow.

# ğŸ—„ï¸ Data Verification
To verify the integrity of the data lake without using the web UI, use the following CLI commands:

**List all archived files:**
```
docker exec -it uk_gas_prices-airflow-scheduler-1 python3 -c "from airflow.providers.amazon.aws.hooks.s3 import S3Hook; print(S3Hook(aws_conn_id='aws_s3_gas_prices').list_keys(bucket_name='uk-gas-price'))"
```
